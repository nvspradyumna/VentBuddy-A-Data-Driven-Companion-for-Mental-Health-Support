{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdfd50b",
   "metadata": {},
   "source": [
    "## DATA602 - Project: VentBuddy - A Data-Driven Companion for Mental Health Support\n",
    "Team Members:\n",
    "- Venkata Sai Pradyumna Nadella - 122096059\n",
    "- Venkata Sai Sri Pujya Kothapalli - 121985248\n",
    "- Mridhula Senthilkumar - 121944900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407175af",
   "metadata": {},
   "source": [
    "## Data Cumulation\n",
    "\n",
    "\n",
    "In this notebook, we will be collecting and cumulating data from multiple folders into a single dataset for further analysis and model training.\n",
    "\n",
    "Below are the steps followed:\n",
    "1. Import necessary libraries, define the base directory containing the data folders\n",
    "2. Rename the folders systematically for easier access\n",
    "3. Handle the currupted file.\n",
    "4. Rename the files inside the folders systematically.\n",
    "5. Combine the data from each month and create a single dataset for that year (1 file per category) and then combine the data from all years into a final dataset for each category to be used from further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84075b00",
   "metadata": {},
   "source": [
    "### 1. Importing required libraries and defining base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571eba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prady\\Desktop\\UMCP_Courses\\DATA602\\Project\\VentBuddy-A-Data-Driven-Companion-for-Mental-Health-Support\n",
      "c:\\Users\\prady\\Desktop\\UMCP_Courses\\DATA602\\Project\\VentBuddy-A-Data-Driven-Companion-for-Mental-Health-Support\\OriginalRedditDataSet\\Raw_Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "import hashlib\n",
    "import ast\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"OriginalRedditDataSet/Raw_Data/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b5e5d",
   "metadata": {},
   "source": [
    "### 2. Renaming Folders Systematically\n",
    "In this step, we will rename the folders and files in a systematic way to ensure consistency and ease of access.\n",
    "* After renaming the folder for the first time, no need of renaming again. So, when the code is run for the second time, it will skip the renamed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "month_map = {\n",
    "    \"jan\": \"Jan\",\n",
    "    \"feb\": \"Feb\",\n",
    "    \"mar\": \"Mar\",\n",
    "    \"apr\": \"Apr\",\n",
    "    \"may\": \"May\",\n",
    "    \"jun\": \"Jun\",\n",
    "    \"june\": \"Jun\",\n",
    "    \"jul\": \"Jul\",\n",
    "    \"aug\": \"Aug\",\n",
    "    \"sep\": \"Sep\",\n",
    "    \"sept\": \"Sep\",\n",
    "    \"oct\": \"Oct\",\n",
    "    \"nov\": \"Nov\",\n",
    "    \"dec\": \"Dec\"\n",
    "}\n",
    "\n",
    "def normalize_folder_name(folder_name, year):\n",
    "    name = folder_name.strip().lower()\n",
    "    name = re.sub(r\"\\s*(\\d+)$\", r\"\\1\", name)\n",
    "    month_key = None\n",
    "    for k in month_map:\n",
    "        if k in name:\n",
    "            month_key = k\n",
    "            break\n",
    "    if not month_key:\n",
    "        return None\n",
    "    month_std = month_map[month_key]\n",
    "    year_suffix = str(year)[-2:]\n",
    "    return f\"{month_std} {year_suffix}\"\n",
    "\n",
    "for year_folder in sorted(base_dir.iterdir()):\n",
    "    if year_folder.is_dir():\n",
    "        year = year_folder.name\n",
    "        print(\"\\nProcessing:\",year_folder.name)\n",
    "        for subfolder in year_folder.iterdir():\n",
    "            if subfolder.is_dir():\n",
    "                new_name = normalize_folder_name(subfolder.name, year)\n",
    "                if new_name:\n",
    "                    new_path = subfolder.parent / new_name\n",
    "                    if subfolder != new_path:\n",
    "                        subfolder.rename(new_path)\n",
    "                        print(\"- Renamed: \"+subfolder.name+\" to \"+new_name)\n",
    "                    else:\n",
    "                        print(\"- No change needed: \",subfolder.name)\n",
    "                else:\n",
    "                    print(\"Skipped unrecognized: \",subfolder.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bad3b4",
   "metadata": {},
   "source": [
    "### 3. Handling the corrupted file\n",
    "The file depmay21.numbers is the lone file which is of type .numbers and is corrupted.\n",
    "So, it was manually converted to .csv and added to the folder.\n",
    "The data in this file is also not consistent with other files. \n",
    "Hence, this is handled separately.\n",
    "* So, the code is commented after execution as it is a one time process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(\"2021\\\\May 21\\\\depmay21_old.csv\")\n",
    "\n",
    "def safe_eval(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}\n",
    "\n",
    "df_d_parsed = df['d_'].apply(safe_eval)\n",
    "expanded_df = df_d_parsed.apply(pd.Series)\n",
    "\n",
    "df=expanded_df\n",
    "\n",
    "df['created'] = pd.to_datetime(df['created'], unit='s', utc=True)\n",
    "df['created'] = df['created'].dt.tz_convert('US/Eastern')\n",
    "df['created'] = df['created'].dt.strftime(\"%#m/%#d/%Y %H:%M\")\n",
    "\n",
    "df.rename(columns={'created': 'timestamp'}, inplace=True)\n",
    "\n",
    "df['Unnamed: 0'] = range(len(df))\n",
    "df = df[['Unnamed: 0', 'author', 'created_utc', 'score', 'selftext', 'subreddit', 'title', 'timestamp']]\n",
    "\n",
    "df.to_csv(\"2021\\\\May 21\\\\depmay21.csv\", index=False)\n",
    "print(\"Updated file saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f4606",
   "metadata": {},
   "source": [
    "### 4. Renaming all the filenames systematically\n",
    "In this step, we will rename all the filenames in a systematic way to ensure consistency and ease of access.\n",
    "This is an important step as some file have camel case, some have underscores, some have hyphens etc.\n",
    "In order to read the files easily, we will be standardizing the filenames.\n",
    "* After renaming the files for the first time, no need of renaming again. So, when the code is run for the second time, it will skip the renamed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5338fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "apply_rename = True\n",
    "only_csv = True\n",
    "log_path = Path(\"../data/Processed/rename_log.csv\")\n",
    "log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "month_keys = {\n",
    "    \"jan\": \"jan\", \"january\": \"jan\",\n",
    "    \"feb\": \"feb\", \"february\": \"feb\",\n",
    "    \"mar\": \"mar\", \"march\": \"mar\",\n",
    "    \"apr\": \"apr\", \"april\": \"apr\",\n",
    "    \"may\": \"may\",\n",
    "    \"jun\": \"jun\", \"june\": \"jun\",\n",
    "    \"jul\": \"jul\", \"july\": \"jul\",\n",
    "    \"aug\": \"aug\", \"august\": \"aug\",\n",
    "    \"sep\": \"sep\", \"sept\": \"sep\", \"september\": \"sep\",\n",
    "    \"oct\": \"oct\", \"october\": \"oct\",\n",
    "    \"nov\": \"nov\", \"november\": \"nov\",\n",
    "    \"dec\": \"dec\", \"december\": \"dec\"\n",
    "}\n",
    "\n",
    "year_re = re.compile(r\"(20\\d{2}|\\d{2})\")\n",
    "alpha_prefix_re = re.compile(r\"^[a-zA-Z]\")\n",
    "\n",
    "first_letter_map = {\n",
    "    \"a\": \"anx\",\n",
    "    \"d\": \"dep\",\n",
    "    \"m\": \"mh\",\n",
    "    \"s\": \"sw\",\n",
    "    \"l\": \"lon\"\n",
    "}\n",
    "\n",
    "def first_letter_category(stem_lower):\n",
    "    if not stem_lower:\n",
    "        return None\n",
    "    first = stem_lower[0]\n",
    "    return first_letter_map.get(first, None)\n",
    "\n",
    "def extract_month(stem_lower, parent_month_name=None):\n",
    "    for k in month_keys:\n",
    "        if k in stem_lower:\n",
    "            return month_keys[k]\n",
    "    if parent_month_name:\n",
    "        pm = parent_month_name.lower()\n",
    "        for k in month_keys:\n",
    "            if k in pm:\n",
    "                return month_keys[k]\n",
    "    return None\n",
    "\n",
    "def extract_year(stem_lower, parent_month_name=None, parent_year_folder=None):\n",
    "    m = year_re.search(stem_lower)\n",
    "    if m:\n",
    "        y = m.group(0)\n",
    "        return y[-2:]\n",
    "    if parent_month_name:\n",
    "        m2 = year_re.search(parent_month_name)\n",
    "        if m2:\n",
    "            return m2.group(0)[-2:]\n",
    "    if parent_year_folder:\n",
    "        if parent_year_folder.isdigit() and len(parent_year_folder) == 4:\n",
    "            return parent_year_folder[-2:]\n",
    "        m3 = year_re.search(parent_year_folder)\n",
    "        if m3:\n",
    "            return m3.group(0)[-2:]\n",
    "    return None\n",
    "\n",
    "rename_records = []\n",
    "skipped = []\n",
    "\n",
    "if not base_dir.exists():\n",
    "    raise FileNotFoundError(f\"Base directory not found: {base_dir}\")\n",
    "\n",
    "planned_count = 0\n",
    "for year_dir in sorted([p for p in base_dir.iterdir() if p.is_dir()]):\n",
    "    for month_dir in sorted([m for m in year_dir.iterdir() if m.is_dir()]):\n",
    "        for file in sorted([f for f in month_dir.iterdir() if f.is_file()]):\n",
    "            if only_csv and file.suffix.lower() != \".csv\":\n",
    "                skipped.append((file, \"non-csv\"))\n",
    "                rename_records.append([year_dir.name, month_dir.name, file.name, \"\", \"skipped\", \"non-csv\"])\n",
    "                continue\n",
    "            \n",
    "            stem = file.stem\n",
    "            stem_lower = stem.lower()\n",
    "                        \n",
    "            category = first_letter_category(stem_lower)\n",
    "            month_extracted = extract_month(stem_lower, parent_month_name=month_dir.name)\n",
    "            year_extracted = extract_year(stem_lower, parent_month_name=month_dir.name, parent_year_folder=year_dir.name)\n",
    "            \n",
    "            reasons = []\n",
    "            if category is None:\n",
    "                reasons.append(\"category-not-detected\")\n",
    "            if month_extracted is None:\n",
    "                reasons.append(\"month-not-detected\")\n",
    "            if year_extracted is None:\n",
    "                reasons.append(\"year-not-detected\")\n",
    "            \n",
    "            if reasons:\n",
    "                reason_text = \";\".join(reasons)\n",
    "                skipped.append((file, reason_text))\n",
    "                rename_records.append([year_dir.name, month_dir.name, file.name, \"\", \"skipped\", reason_text])\n",
    "                continue\n",
    "            \n",
    "            new_name_stem = f\"{category}{month_extracted}{year_extracted}\"\n",
    "            new_name = new_name_stem + file.suffix.lower()\n",
    "            new_path = file.with_name(new_name)\n",
    "            \n",
    "            counter = 1\n",
    "            while new_path.exists() and new_path.resolve() != file.resolve():\n",
    "                new_name = f\"{new_name_stem}_{counter}{file.suffix.lower()}\"\n",
    "                new_path = file.with_name(new_name)\n",
    "                counter += 1\n",
    "            \n",
    "            planned_count += 1\n",
    "            try:\n",
    "                if apply_rename:\n",
    "                    file.rename(new_path)\n",
    "                    print(f\"RENAMED: {file.relative_to(base_dir)}  ->  {new_path.relative_to(base_dir)}\")\n",
    "                    rename_records.append([year_dir.name, month_dir.name, file.name, new_path.name, \"renamed\", \"\"])\n",
    "                else:\n",
    "                    print(f\"WOULD RENAME: {file.relative_to(base_dir)}  ->  {new_path.relative_to(base_dir)}\")\n",
    "                    rename_records.append([year_dir.name, month_dir.name, file.name, new_path.name, \"planned\", \"\"])\n",
    "            except Exception as e:\n",
    "                err = f\"rename-failed:{e}\"\n",
    "                print(f\"ERROR renaming {file}: {err}\")\n",
    "                skipped.append((file, err))\n",
    "                rename_records.append([year_dir.name, month_dir.name, file.name, \"\", \"failed\", err])\n",
    "\n",
    "print(f\"\\nSummary: attempted renames = {planned_count}, skipped = {len(skipped)}\")\n",
    "\n",
    "with open(log_path, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow([\"year_folder\", \"month_folder\", \"old_name\", \"new_name\", \"status\", \"reason\"])\n",
    "    writer.writerows(rename_records)\n",
    "\n",
    "print(f\"Rename log saved to: {log_path}\")\n",
    "\n",
    "if skipped:\n",
    "    print(\"\\n=== SKIPPED FILES (relative_path : reason) ===\")\n",
    "    for fpath, reason in skipped:\n",
    "        try:\n",
    "            print(f\"{fpath.relative_to(base_dir)}  :  {reason}\")\n",
    "        except Exception:\n",
    "            print(f\"{fpath}  :  {reason}\")\n",
    "else:\n",
    "    print(\"\\nNo files were skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a7c1c",
   "metadata": {},
   "source": [
    "### 5. Combining data from each month into a single dataset for that year and then combining data from all years\n",
    "In this step, we will combine the data from each month into a single dataset for that year and then combine data from all years.\n",
    "This will result in a final dataset for each category to be used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-step merge:\n",
    "# Step A: create per-year per-category files inside each year folder\n",
    "# Step B: combine per-year files across years into final per-category files\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "per_year_log = Path(\"../Cumulated_Data/Processed/per_year_merge_log.csv\")\n",
    "final_log = Path(\"../Cumulated_Data/Processed/final_combine_log.csv\")\n",
    "final_outdir = Path(\"../Cumulated_Data/Combined_by_category\")\n",
    "\n",
    "dedupe_per_year = True\n",
    "dedupe_final = True\n",
    "categories = [\"anx\", \"dep\", \"sw\", \"lon\", \"mh\"]\n",
    "\n",
    "monthly_pattern = re.compile(r\"^(?P<cat>[a-z]+)(?P<mon>[a-z]{3})(?P<yy>\\d{2})\\.csv$\", re.IGNORECASE)\n",
    "per_year_pattern = re.compile(r\"^(?P<cat>[a-z]+)[_]?[_]?(?P<year>\\d{4})\\.csv$\", re.IGNORECASE)\n",
    "\n",
    "final_outdir.mkdir(parents=True, exist_ok=True)\n",
    "per_year_log.parent.mkdir(parents=True, exist_ok=True)\n",
    "final_log.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_cat(pref: str):\n",
    "    p = pref.lower()\n",
    "    if p.startswith(\"an\"):\n",
    "        return \"anx\"\n",
    "    if p.startswith(\"de\"):\n",
    "        return \"dep\"\n",
    "    if p.startswith(\"su\") or p.startswith(\"sw\"):\n",
    "        return \"sw\"\n",
    "    if p.startswith(\"lo\"):\n",
    "        return \"lon\"\n",
    "    if p.startswith(\"m\"):\n",
    "        return \"mh\"\n",
    "    return None\n",
    "\n",
    "# STEP A: Per-year merges\n",
    "per_year_records = []\n",
    "all_skipped = []\n",
    "\n",
    "for year_dir in sorted([p for p in base_dir.iterdir() if p.is_dir()]):\n",
    "    year = year_dir.name\n",
    "    print(f\"\\n=== Processing Year: {year} ===\")\n",
    "    monthly_files = sorted(year_dir.rglob(\"*.csv\"))\n",
    "    \n",
    "    cat_to_files = {c: [] for c in categories}\n",
    "    skipped_discovery = []\n",
    "    for f in monthly_files:\n",
    "        m = monthly_pattern.match(f.name)\n",
    "        if not m:\n",
    "            if per_year_pattern.match(f.name):\n",
    "                continue\n",
    "            skipped_discovery.append((f, \"pattern-mismatch\"))\n",
    "            continue\n",
    "        cat_raw = m.group(\"cat\")\n",
    "        cat = normalize_cat(cat_raw)\n",
    "        yy = m.group(\"yy\")\n",
    "        if year[-2:] != yy:\n",
    "            skipped_discovery.append((f, f\"year-mismatch (file yy={yy})\"))\n",
    "            continue\n",
    "        if cat:\n",
    "            cat_to_files[cat].append(f)\n",
    "        else:\n",
    "            skipped_discovery.append((f, \"unrecognized-category\"))\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(f\"  {cat}: {len(cat_to_files[cat])} monthly files found\")\n",
    "    if skipped_discovery:\n",
    "        print(f\" - {len(skipped_discovery)} files skipped during discovery in {year}:\")\n",
    "        for sf, reason in skipped_discovery[:20]:\n",
    "            print(f\"     - {sf.relative_to(base_dir)} ({reason})\")\n",
    "        if len(skipped_discovery) > 20:\n",
    "            print(f\"     ... and {len(skipped_discovery)-20} more\")\n",
    "    all_skipped.extend(skipped_discovery)\n",
    "    \n",
    "    for cat in categories:\n",
    "        files_list = cat_to_files.get(cat, [])\n",
    "        out_file = year_dir / f\"{cat}_{year}.csv\"\n",
    "        if not files_list:\n",
    "            per_year_records.append([year, cat, 0, 0, 0, str(out_file), \"no_files\"])\n",
    "            print(f\"   - {cat}: no monthly files -> skipped creating {out_file.name}\")\n",
    "            continue\n",
    "        \n",
    "        dfs = []\n",
    "        rows_read = 0\n",
    "        failed_reads = []\n",
    "        for f in files_list:\n",
    "            try:\n",
    "                if f.stat().st_size == 0:\n",
    "                    failed_reads.append((f, \"empty\"))\n",
    "                    print(f\" - Skipped empty: {f.relative_to(base_dir)}\")\n",
    "                    continue\n",
    "                df = pd.read_csv(f, low_memory=False)\n",
    "                dfs.append(df)\n",
    "                rows_read += len(df)\n",
    "                print(f\"     read: {f.relative_to(base_dir)} ({len(df)} rows)\")\n",
    "            except pd.errors.EmptyDataError:\n",
    "                failed_reads.append((f, \"EmptyDataError\"))\n",
    "                print(f\" - EmptyDataError: {f.relative_to(base_dir)}\")\n",
    "            except pd.errors.ParserError:\n",
    "                failed_reads.append((f, \"ParserError\"))\n",
    "                print(f\" - ParserError: {f.relative_to(base_dir)}\")\n",
    "            except Exception as e:\n",
    "                failed_reads.append((f, f\"error:{e}\"))\n",
    "                print(f\" - Error reading {f.relative_to(base_dir)}: {e}\")\n",
    "        \n",
    "        if not dfs:\n",
    "            per_year_records.append([year, cat, len(files_list), 0, 0, str(out_file), \"all_reads_failed\"])\n",
    "            print(f\"   - {cat}: no readable monthly CSVs; skipping {out_file.name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "        except Exception as e:\n",
    "            per_year_records.append([year, cat, len(files_list), rows_read, 0, str(out_file), f\"concat_failed:{e}\"])\n",
    "            print(f\"   - {cat}: concat failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        before = len(combined)\n",
    "        if dedupe_per_year:\n",
    "            combined = combined.drop_duplicates()\n",
    "        after = len(combined)\n",
    "        \n",
    "        try:\n",
    "            combined.to_csv(out_file, index=False)\n",
    "            per_year_records.append([year, cat, len(files_list), before, after, str(out_file), \"merged\"])\n",
    "            print(f\"   → Wrote {out_file.relative_to(base_dir)} : {after} rows (before dedupe {before})\")\n",
    "        except Exception as e:\n",
    "            per_year_records.append([year, cat, len(files_list), before, after, str(out_file), f\"write_failed:{e}\"])\n",
    "            print(f\"   - {cat}: failed to write {out_file.name}: {e}\")\n",
    "\n",
    "with open(per_year_log, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow([\"year\", \"category\", \"n_month_files\", \"rows_before_dedupe\", \"rows_after_dedupe\", \"out_path\", \"status\"])\n",
    "    writer.writerows(per_year_records)\n",
    "print(f\"\\nPer-year merge log saved to: {per_year_log}\")\n",
    "\n",
    "if all_skipped:\n",
    "    print(f\"\\n=== Skipped files during per-year processing ({len(all_skipped)}) ===\")\n",
    "    for sf, reason in all_skipped:\n",
    "        try:\n",
    "            print(f\" - {sf.relative_to(base_dir)} : {reason}\")\n",
    "        except Exception:\n",
    "            print(f\" - {sf} : {reason}\")\n",
    "else:\n",
    "    print(\"\\nNo files skipped during per-year discovery.\")\n",
    "\n",
    "# STEP B: Final per-category combine across years\n",
    "print(\"\\n\\n=== STEP B: Combining per-year files across years into final per-category files ===\")\n",
    "final_records = []\n",
    "for cat in categories:\n",
    "    per_year_files = []\n",
    "    for year_dir in sorted([p for p in base_dir.iterdir() if p.is_dir()]):\n",
    "        candidate = year_dir / f\"{cat}_{year_dir.name}.csv\"\n",
    "        if candidate.exists():\n",
    "            per_year_files.append(candidate)\n",
    "    if not per_year_files:\n",
    "        print(f\"  - {cat}: no per-year files found -> skipping final combine\")\n",
    "        final_records.append([cat, 0, 0, 0, str(final_outdir / f\"{cat}_data.csv\"), \"no_files\"])\n",
    "        continue\n",
    "    \n",
    "    dfs = []\n",
    "    rows_read = 0\n",
    "    failed = []\n",
    "    for f in per_year_files:\n",
    "        try:\n",
    "            if f.stat().st_size == 0:\n",
    "                failed.append((f, \"empty\"))\n",
    "                print(f\" - empty per-year file skipped: {f.relative_to(base_dir)}\")\n",
    "                continue\n",
    "            df = pd.read_csv(f, low_memory=False)\n",
    "            dfs.append(df)\n",
    "            rows_read += len(df)\n",
    "            print(f\"    read per-year: {f.relative_to(base_dir)} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            failed.append((f, f\"read-error:{e}\"))\n",
    "            print(f\" - failed to read {f.relative_to(base_dir)}: {e}\")\n",
    "    if not dfs:\n",
    "        final_records.append([cat, len(per_year_files), 0, 0, str(final_outdir / f\"{cat}_data.csv\"), \"all_reads_failed\"])\n",
    "        print(f\"  - {cat}: no readable per-year files; skipping final output\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        combined_final = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    except Exception as e:\n",
    "        final_records.append([cat, len(per_year_files), rows_read, 0, str(final_outdir / f\"{cat}_data.csv\"), f\"concat_failed:{e}\"])\n",
    "        print(f\"  - {cat}: concat failed: {e}\")\n",
    "        continue\n",
    "    \n",
    "    before_final = len(combined_final)\n",
    "    if dedupe_final:\n",
    "        combined_final = combined_final.drop_duplicates()\n",
    "    after_final = len(combined_final)\n",
    "    \n",
    "    out_final = final_outdir / f\"{cat}_data.csv\"\n",
    "    try:\n",
    "        combined_final.to_csv(out_final, index=False)\n",
    "        final_records.append([cat, len(per_year_files), before_final, after_final, str(out_final), \"merged\"])\n",
    "        print(f\"  → Wrote final: {out_final} : {after_final} rows (before dedupe {before_final})\")\n",
    "    except Exception as e:\n",
    "        final_records.append([cat, len(per_year_files), before_final, after_final, str(out_final), f\"write_failed:{e}\"])\n",
    "        print(f\"  - {cat}: failed to write final output: {e}\")\n",
    "\n",
    "with open(final_log, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    writer = csv.writer(fh)\n",
    "    writer.writerow([\"category\", \"n_per_year_files\", \"rows_before_dedupe\", \"rows_after_dedupe\", \"out_path\", \"status\"])\n",
    "    writer.writerows(final_records)\n",
    "print(f\"\\nFinal combine log saved to: {final_log}\")\n",
    "\n",
    "print(\"\\nAll done. Per-year files are created inside each year folder and final per-category files are in:\")\n",
    "print(final_outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95003067",
   "metadata": {},
   "source": [
    "### At this point, the data is successfully cumulated and ready for further analysis and preprocessing. This will be carried out in the DataPreprocessing.ipynb notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
